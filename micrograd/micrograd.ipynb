{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "  def __init__(self, data, _children=(), _op='', label=''):\n",
    "    self.data = data\n",
    "    self._prev = set(_children)\n",
    "    self.label = label\n",
    "    self._backward = lambda: None\n",
    "    self.grad = 0.0\n",
    "    self._op = _op\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f'Value:{self.data}'\n",
    "\n",
    "  def __add__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    self = self if isinstance(self, Value) else Value(self)\n",
    "    out = Value(self.data + other.data, (self, other), '+')\n",
    "    def _backward():\n",
    "      self.grad += 1.0 * out.grad\n",
    "      other.grad += 1.0 * out.grad\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "  def __sub__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    self = self if isinstance(self, Value) else Value(self)\n",
    "    out = Value(self.data - other.data, (self, other), '-')\n",
    "    def _backward():\n",
    "      self.grad += out.grad * 1.0\n",
    "      other.grad += out.grad * 1.0\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "  def __rsub__(self, other):\n",
    "    return self * other\n",
    "  \n",
    "  def __pow__(self, other):\n",
    "    assert isinstance(other, (int, float)), 'only int/float support'\n",
    "    out = Value(self.data**other, (self,), f'**{other}')\n",
    "    def _backward():\n",
    "      self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "  def __truediv__(self, other):\n",
    "    out = Value(self.data / other.data, (self, other), '/')\n",
    "    def _backward():\n",
    "      # dout / dself, dout / dother\n",
    "      self.grad += (1 / other.data) * out.grad\n",
    "      other.grad += self.data * (-1 * other.data ** -2) * out.grad\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "\n",
    "  def __mul__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "    def _backward():\n",
    "      # dout / dself, dout / dother\n",
    "      self.grad += other.data * out.grad\n",
    "      other.grad += self.data * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "  def __rmul__(self, other):\n",
    "    return self * other\n",
    "\n",
    "  def exp(self):\n",
    "    out = Value(math.exp(self.data), (self,), 'exp')\n",
    "    def _backward():\n",
    "      self.grad += out.data * out.grad\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "  \n",
    "  def tanh(self):\n",
    "    x = self.data\n",
    "    t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "    out = Value(t, (self, ), 'tanh')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += (1 - t**2) * out.grad\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "\n",
    "  def backward(self):\n",
    "    topo = []\n",
    "    visited = set()\n",
    "    def build_topo(v):\n",
    "      if v not in visited:\n",
    "        visited.add(v)\n",
    "        for child in v._prev:\n",
    "          build_topo(child)\n",
    "        topo.append(v)\n",
    "    build_topo(self)\n",
    "    self.grad = 1.0\n",
    "    for node in reversed(topo):\n",
    "      node._backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Value:0.6791272391406387, 1.0775723861144275, 0.0)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Single Neuron\n",
    "\n",
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "\n",
    "# weights\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "\n",
    "# bias\n",
    "b = Value(6.827492387982534, label='b')\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label='x1w1'\n",
    "x2w2 = x2*w2; x2w2.label='x2w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label='x1w1x2w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "\n",
    "# activation\n",
    "out = n.tanh()\n",
    "\n",
    "# Custom Activation \n",
    "# e = (2 * n).exp()\n",
    "# out = (e - 1) / (e + 1)\n",
    "\n",
    "# Back propogation\n",
    "out.backward()\n",
    "\n",
    "out, w1.grad, w2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        # nin: number of inputs to the neuron\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "    \n",
    "\n",
    "    def __call__(self, x):\n",
    "        # w*x + b\n",
    "        out = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = out.tanh()\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "    \n",
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        # nin: number of inputs to a single neuron\n",
    "        # nout: number of neurons in a single layer\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        out = [layer(x) for layer in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts):\n",
    "        # nin: number of input to a single neuron\n",
    "        # nouts: list of number of neurons in each layer\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x) \n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "# single row is one training example \n",
    "# number of col in single training example is the number of input of a single neuron\n",
    "y = [1.0, -1.0, -1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLP(3, [4, 4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9886580389864231 [Value:-0.042787656941246636, Value:0.7491121426348154, Value:-0.026880477999085974, Value:0.651870917482532]\n",
      "1 0.6234818970705979 [Value:-0.41631864796593554, Value:0.5212647955153462, Value:-0.2885503932477418, Value:0.3085163916866187]\n",
      "2 0.3693916435092679 [Value:-0.3517527674000081, Value:0.39521633068638035, Value:-0.11284058218672631, Value:0.27700665895652765]\n",
      "3 0.21110605882119604 [Value:-0.33711535967862505, Value:0.2153069976902008, Value:-0.10291023575214876, Value:0.2012751182163765]\n",
      "4 0.12202488246747817 [Value:-0.22716123026922416, Value:0.13150254441902012, Value:-0.023234781863334993, Value:0.22932484307884649]\n",
      "5 0.08037694281526947 [Value:-0.20713640622289567, Value:0.022310861673561864, Value:-0.03808701880374307, Value:0.18847561243396857]\n",
      "6 0.06309557521984092 [Value:-0.12048466492087737, Value:0.009905304922580252, Value:0.017237630331072985, Value:0.2195080175613292]\n",
      "7 0.05541507607638892 [Value:-0.1621162028864679, Value:-0.06012974927008599, Value:-0.02947181907671239, Value:0.15700075786787981]\n",
      "8 0.05211794475538284 [Value:-0.06392976604096351, Value:-0.01581545217566824, Value:0.0445573922191836, Value:0.2139986916792612]\n",
      "9 0.052150729869809585 [Value:-0.16732226767116237, Value:-0.09488125578595608, Value:-0.044475736877778685, Value:0.1147756278172681]\n",
      "10 0.05587087015054167 [Value:-0.012018157529062976, Value:0.0009878401062708196, Value:0.07669604501431393, Value:0.2232558507435697]\n",
      "11 0.06577266779594691 [Value:-0.20096997078488094, Value:-0.12448044068403837, Value:-0.07959817402599632, Value:0.05960276182783673]\n",
      "12 0.08228517111416384 [Value:0.05436396462606657, Value:0.04115666200047717, Value:0.12149163434514251, Value:0.2507501593647037]\n",
      "13 0.10976149806368757 [Value:-0.25586751368778193, Value:-0.16228727409872845, Value:-0.13356706988121297, Value:-0.010769958768335863]\n",
      "14 0.14071094751874236 [Value:0.13002087111699714, Value:0.09527382981132154, Value:0.1747336113780851, Value:0.29016647463236556]\n",
      "15 0.17745801621681653 [Value:-0.3087353267569312, Value:-0.1997336049249743, Value:-0.1893788657144892, Value:-0.07989146708801378]\n",
      "16 0.1961047127905513 [Value:0.18068535367588892, Value:0.13950098150896323, Value:0.2106907064444668, Value:0.3156048449145508]\n",
      "17 0.20858422339812982 [Value:-0.3232922535628321, Value:-0.21321238510224558, Value:-0.21358035301288275, Value:-0.11397479471488167]\n",
      "18 0.1943619165947395 [Value:0.1836831487443588, Value:0.1502235942554469, Value:0.209673101052536, Value:0.306744975323289]\n",
      "19 0.1812227438202981 [Value:-0.2987276907562151, Value:-0.19931669697008425, Value:-0.2006909031324914, Value:-0.10945559055871829]\n"
     ]
    }
   ],
   "source": [
    "for k in range(20):\n",
    "\n",
    "    ypred = [nn(e) for e in x] # forward pass\n",
    "    loss = sum(((orig - pred)**2 for orig, pred in zip(y, ypred)), Value(0.0))\n",
    "\n",
    "    for p in nn.parameters():\n",
    "        p.grad = 0.0\n",
    "    loss.backward()\n",
    "\n",
    "    for p in nn.parameters():\n",
    "        p.data += -0.05 * p.grad \n",
    "\n",
    "    print(k, loss.data, ypred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.8526652607756128, 0.013390524647337244)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.layers[0].neurons[0].w[0].data, nn.layers[0].neurons[0].w[0].grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7669226255009629"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.4569054185925154 * -0.01 + -0.771491679686888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
